{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fff3e62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "from datetime import date,datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a66deae-edd8-49d3-af19-8800f89fbc4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Make sure we have a GPU allocated\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(device) # The output should be 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3bc4ff",
   "metadata": {},
   "source": [
    "## Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c11fcd1-7bee-4e91-8e3d-180581829e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_split1 = 255000 #spaeter 250000\n",
    "n_split2 = 255000\n",
    "n_gesamt = n_split1 + n_split2\n",
    "batchsize = 256 #baseline 256 #1000\n",
    "\n",
    "training_percentage = 80 \n",
    "validation_percentage = 10 \n",
    "test_percentage = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abe41468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [28.150738] torch.Size([256, 1])\n",
      "32000 [39.779995] torch.Size([256, 1])\n",
      "64000 [30.451612] torch.Size([256, 1])\n",
      "96000 [92.544975] torch.Size([256, 1])\n",
      "128000 [56.029053] torch.Size([256, 1])\n",
      "160000 [40.94045] torch.Size([256, 1])\n",
      "192000 [35.8835] torch.Size([256, 1])\n",
      "pion_orig:\n",
      "204032\n",
      "204000 [57.27977] torch.Size([256, 1])\n",
      "pion_orig_validation:\n",
      "25600\n",
      "pion_orig_test:\n",
      "25600\n",
      "255000 [21.80484] torch.Size([256, 1])\n",
      "287000 [56.13993] torch.Size([256, 1])\n",
      "319000 [28.610886] torch.Size([256, 1])\n",
      "351000 [59.066257] torch.Size([256, 1])\n",
      "383000 [12.57898] torch.Size([256, 1])\n",
      "415000 [20.492687] torch.Size([256, 1])\n",
      "447000 [32.287186] torch.Size([256, 1])\n",
      "pion_trafo:\n",
      "204032\n",
      "459000 [55.703934] torch.Size([256, 1])\n",
      "pion_trafo_validation:\n",
      "25600\n",
      "pion_trafo_test:\n",
      "25600\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "input_filename =  \"/beegfs/desy/user/diefenbs/shower_data/pion_uniform_510k_PunchThroughCut70.hdf5\"\n",
    "f_in = h5py.File(input_filename, 'r') ## 'r'=readable\n",
    "\n",
    "assert(n_split1+n_split2 <= (f_in['hcal_only']['layers'].shape[0]))\n",
    "\n",
    "output_filename_split1 = f\"/beegfs/desy/user/schreibj/Hadron_classifier/shower_data/pion_{int(n_gesamt/1000)}k_orig.hdf5\" \n",
    "output_filename_split2 = f\"/beegfs/desy/user/schreibj/Hadron_classifier/shower_data/pion_{int(n_gesamt/1000)}k_trafo.hdf5\"\n",
    "output_filename_split1_validation = f\"/beegfs/desy/user/schreibj/Hadron_classifier/shower_data/pion_{int(n_gesamt/1000)}k_orig_validation.hdf5\" \n",
    "output_filename_split2_validation = f\"/beegfs/desy/user/schreibj/Hadron_classifier/shower_data/pion_{int(n_gesamt/1000)}k_trafo_validation.hdf5\"\n",
    "output_filename_split1_test = f\"/beegfs/desy/user/schreibj/Hadron_classifier/shower_data/pion_{int(n_gesamt/1000)}k_orig_test.hdf5\" \n",
    "output_filename_split2_test = f\"/beegfs/desy/user/schreibj/Hadron_classifier/shower_data/pion_{int(n_gesamt/1000)}k_trafo_test.hdf5\"\n",
    "\n",
    "\n",
    "f_out = h5py.File(output_filename_split1,'w-')\n",
    "grp = f_out.create_group(\"hcal_only\")\n",
    "dset = grp.create_dataset('layers', shape=(0, 48, 48, 48), maxshape=(None, 48, 48, 48), \n",
    "                          chunks=(100, 48, 48, 48), dtype='f8', compression=\"gzip\", compression_opts=1)\n",
    "dsetE = grp.create_dataset('energy', shape=(0, 1), maxshape=(None, 1), chunks=(100, 1), \n",
    "                           dtype='f4', compression=\"gzip\", compression_opts=1)\n",
    "dsetL = grp.create_dataset('label', shape=(0, 1), maxshape=(None, 1), chunks=(100, 1), \n",
    "                           dtype='f2', compression=\"gzip\", compression_opts=1)\n",
    "\n",
    "\n",
    "for index in np.arange(0, int(n_split1*training_percentage*0.01), step=batchsize):\n",
    "    \n",
    "    ds1 = f_in['hcal_only']['layers'][index:index+batchsize]\n",
    "    dset.resize(dset.shape[0]+ds1.shape[0], axis=0)\n",
    "    dset[-1*ds1.shape[0]:] = ds1\n",
    "\n",
    "    ds1E = f_in['hcal_only']['energy'][index:index+batchsize]   \n",
    "    dsetE.resize(dsetE.shape[0]+ds1E.shape[0], axis=0)\n",
    "    dsetE[-1*ds1E.shape[0]:] = ds1E\n",
    "    \n",
    "    ds1L = torch.zeros((batchsize,1)) \n",
    "    dsetL.resize(dsetL.shape[0]+ds1L.shape[0], axis=0)\n",
    "    dsetL[-1*ds1L.shape[0]:] = ds1L\n",
    "    \n",
    "\n",
    "    if index%1000 == 0:\n",
    "        print(index, ds1E[0],ds1L.shape)\n",
    "\n",
    "print('pion_orig:')        \n",
    "print(f_out['hcal_only']['layers'].shape[0])\n",
    "\n",
    "f_out.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "f_out = h5py.File(output_filename_split1_validation,'w-')\n",
    "grp = f_out.create_group(\"hcal_only\")\n",
    "dset = grp.create_dataset('layers', shape=(0, 48, 48, 48), maxshape=(None, 48, 48, 48), \n",
    "                          chunks=(100, 48, 48, 48), dtype='f8', compression=\"gzip\", compression_opts=1)\n",
    "dsetE = grp.create_dataset('energy', shape=(0, 1), maxshape=(None, 1), chunks=(100, 1), \n",
    "                           dtype='f4', compression=\"gzip\", compression_opts=1)\n",
    "dsetL = grp.create_dataset('label', shape=(0, 1), maxshape=(None, 1), chunks=(100, 1), \n",
    "                           dtype='f2', compression=\"gzip\", compression_opts=1)\n",
    "\n",
    "\n",
    "for index in np.arange(int(n_split1*training_percentage*0.01), int(n_split1*(training_percentage + validation_percentage)*0.01), step=batchsize):    \n",
    "    \n",
    "    ds1 = f_in['hcal_only']['layers'][index:index+batchsize]\n",
    "    dset.resize(dset.shape[0]+ds1.shape[0], axis=0)\n",
    "    dset[-1*ds1.shape[0]:] = ds1\n",
    "\n",
    "    ds1E = f_in['hcal_only']['energy'][index:index+batchsize]   \n",
    "    dsetE.resize(dsetE.shape[0]+ds1E.shape[0], axis=0)\n",
    "    dsetE[-1*ds1E.shape[0]:] = ds1E\n",
    "    \n",
    "    ds1L = torch.zeros((batchsize,1)) \n",
    "    dsetL.resize(dsetL.shape[0]+ds1L.shape[0], axis=0)\n",
    "    dsetL[-1*ds1L.shape[0]:] = ds1L\n",
    "    \n",
    "\n",
    "    if index%1000 == 0:\n",
    "        print(index, ds1E[0],ds1L.shape)\n",
    "\n",
    "print('pion_orig_validation:')        \n",
    "print(f_out['hcal_only']['layers'].shape[0])\n",
    "\n",
    "f_out.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "f_out = h5py.File(output_filename_split1_test,'w-')\n",
    "grp = f_out.create_group(\"hcal_only\")\n",
    "dset = grp.create_dataset('layers', shape=(0, 48, 48, 48), maxshape=(None, 48, 48, 48), \n",
    "                          chunks=(100, 48, 48, 48), dtype='f8', compression=\"gzip\", compression_opts=1)\n",
    "dsetE = grp.create_dataset('energy', shape=(0, 1), maxshape=(None, 1), chunks=(100, 1), \n",
    "                           dtype='f4', compression=\"gzip\", compression_opts=1)\n",
    "dsetL = grp.create_dataset('label', shape=(0, 1), maxshape=(None, 1), chunks=(100, 1), \n",
    "                           dtype='f2', compression=\"gzip\", compression_opts=1)\n",
    "\n",
    "\n",
    "for index in np.arange(int(n_split1*(training_percentage + validation_percentage)*0.01), n_split1, step=batchsize): \n",
    "    \n",
    "    ds1 = f_in['hcal_only']['layers'][index:index+batchsize]\n",
    "    dset.resize(dset.shape[0]+ds1.shape[0], axis=0)\n",
    "    dset[-1*ds1.shape[0]:] = ds1\n",
    "\n",
    "    ds1E = f_in['hcal_only']['energy'][index:index+batchsize]   \n",
    "    dsetE.resize(dsetE.shape[0]+ds1E.shape[0], axis=0)\n",
    "    dsetE[-1*ds1E.shape[0]:] = ds1E\n",
    "    \n",
    "    ds1L = torch.zeros((batchsize,1)) \n",
    "    dsetL.resize(dsetL.shape[0]+ds1L.shape[0], axis=0)\n",
    "    dsetL[-1*ds1L.shape[0]:] = ds1L\n",
    "    \n",
    "\n",
    "    if index%1000 == 0:\n",
    "        print(index, ds1E[0],ds1L.shape)\n",
    "\n",
    "print('pion_orig_test:')        \n",
    "print(f_out['hcal_only']['layers'].shape[0])\n",
    "\n",
    "f_out.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "f_out = h5py.File(output_filename_split2,'w-')\n",
    "grp = f_out.create_group(\"hcal_only\")\n",
    "dset = grp.create_dataset('layers', shape=(0, 48, 48, 48), maxshape=(None, 48, 48, 48), \n",
    "                          chunks=(100, 48, 48, 48), dtype='f8', compression=\"gzip\", compression_opts=1)\n",
    "dsetE = grp.create_dataset('energy', shape=(0, 1), maxshape=(None, 1), chunks=(100, 1), \n",
    "                           dtype='f4', compression=\"gzip\", compression_opts=1)\n",
    "dsetL = grp.create_dataset('label', shape=(0, 1), maxshape=(None, 1), chunks=(100, 1), \n",
    "                           dtype='f2', compression=\"gzip\", compression_opts=1)\n",
    "\n",
    "\n",
    "for index in np.arange(n_split1, n_split1 + int(n_split2*training_percentage*0.01) ,step=batchsize):\n",
    "    \n",
    "    ds1 = f_in['hcal_only']['layers'][index:index+batchsize]\n",
    "    dset.resize(dset.shape[0]+ds1.shape[0], axis=0)\n",
    "    dset[-1*ds1.shape[0]:] = ds1\n",
    "\n",
    "    ds1E = f_in['hcal_only']['energy'][index:index+batchsize]   \n",
    "    dsetE.resize(dsetE.shape[0]+ds1E.shape[0], axis=0)\n",
    "    dsetE[-1*ds1E.shape[0]:] = ds1E\n",
    "\n",
    "    ds1L = torch.ones((batchsize,1))  \n",
    "    dsetL.resize(dsetL.shape[0]+ds1L.shape[0], axis=0)\n",
    "    dsetL[-1*ds1L.shape[0]:] = ds1L\n",
    "    \n",
    "    if index%1000 == 0:\n",
    "        print(index, ds1E[0], ds1L.shape)        \n",
    "\n",
    "print('pion_trafo:')\n",
    "print(f_out['hcal_only']['layers'].shape[0])\n",
    "\n",
    "f_out.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "f_out = h5py.File(output_filename_split2_validation,'w-')\n",
    "grp = f_out.create_group(\"hcal_only\")\n",
    "dset = grp.create_dataset('layers', shape=(0, 48, 48, 48), maxshape=(None, 48, 48, 48), \n",
    "                          chunks=(100, 48, 48, 48), dtype='f8', compression=\"gzip\", compression_opts=1)\n",
    "dsetE = grp.create_dataset('energy', shape=(0, 1), maxshape=(None, 1), chunks=(100, 1), \n",
    "                           dtype='f4', compression=\"gzip\", compression_opts=1)\n",
    "dsetL = grp.create_dataset('label', shape=(0, 1), maxshape=(None, 1), chunks=(100, 1), \n",
    "                           dtype='f2', compression=\"gzip\", compression_opts=1)\n",
    "\n",
    "\n",
    "for index in np.arange(n_split1 + int(n_split2*training_percentage*0.01) , n_split1 + int(n_split2*(training_percentage + validation_percentage)*0.01) ,step=batchsize):\n",
    "    \n",
    "    ds1 = f_in['hcal_only']['layers'][index:index+batchsize]\n",
    "    dset.resize(dset.shape[0]+ds1.shape[0], axis=0)\n",
    "    dset[-1*ds1.shape[0]:] = ds1\n",
    "\n",
    "    ds1E = f_in['hcal_only']['energy'][index:index+batchsize]   \n",
    "    dsetE.resize(dsetE.shape[0]+ds1E.shape[0], axis=0)\n",
    "    dsetE[-1*ds1E.shape[0]:] = ds1E\n",
    "\n",
    "    ds1L = torch.ones((batchsize,1))  \n",
    "    dsetL.resize(dsetL.shape[0]+ds1L.shape[0], axis=0)\n",
    "    dsetL[-1*ds1L.shape[0]:] = ds1L\n",
    "    \n",
    "    if index%1000 == 0:\n",
    "        print(index, ds1E[0], ds1L.shape)        \n",
    "\n",
    "print('pion_trafo_validation:')\n",
    "print(f_out['hcal_only']['layers'].shape[0])      \n",
    "\n",
    "f_out.close() \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "f_out = h5py.File(output_filename_split2_test,'w-')\n",
    "grp = f_out.create_group(\"hcal_only\")\n",
    "dset = grp.create_dataset('layers', shape=(0, 48, 48, 48), maxshape=(None, 48, 48, 48), \n",
    "                          chunks=(100, 48, 48, 48), dtype='f8', compression=\"gzip\", compression_opts=1)\n",
    "dsetE = grp.create_dataset('energy', shape=(0, 1), maxshape=(None, 1), chunks=(100, 1), \n",
    "                           dtype='f4', compression=\"gzip\", compression_opts=1)\n",
    "dsetL = grp.create_dataset('label', shape=(0, 1), maxshape=(None, 1), chunks=(100, 1), \n",
    "                           dtype='f2', compression=\"gzip\", compression_opts=1)\n",
    "\n",
    "\n",
    "for index in np.arange(n_split1 + int(n_split2*(training_percentage + validation_percentage)*0.01) , n_split1+n_split2 ,step=batchsize):\n",
    "    \n",
    "    ds1 = f_in['hcal_only']['layers'][index:index+batchsize]\n",
    "    dset.resize(dset.shape[0]+ds1.shape[0], axis=0)\n",
    "    dset[-1*ds1.shape[0]:] = ds1\n",
    "\n",
    "    ds1E = f_in['hcal_only']['energy'][index:index+batchsize]   \n",
    "    dsetE.resize(dsetE.shape[0]+ds1E.shape[0], axis=0)\n",
    "    dsetE[-1*ds1E.shape[0]:] = ds1E\n",
    "\n",
    "    ds1L = torch.ones((batchsize,1))  \n",
    "    dsetL.resize(dsetL.shape[0]+ds1L.shape[0], axis=0)\n",
    "    dsetL[-1*ds1L.shape[0]:] = ds1L\n",
    "    \n",
    "    if index%1000 == 0:\n",
    "        print(index, ds1E[0], ds1L.shape)        \n",
    "\n",
    "print('pion_trafo_test:')\n",
    "print(f_out['hcal_only']['layers'].shape[0])\n",
    "\n",
    "f_out.close() \n",
    "\n",
    "end = datetime.now()\n",
    "print(f'total time: {end - start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73172bd-daaa-4f99-be46-9b5e59c804b8",
   "metadata": {},
   "source": [
    "# Define Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8af65516-f1db-4e05-9c2b-b129a4d1e4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_trafo_Gauss_Sigma01(x):\n",
    "    return x*np.random.normal(loc=1.0, scale=0.1, size=x.shape)\n",
    "\n",
    "def test_trafo_Gauss_Sigma025(x):\n",
    "    return x*np.random.normal(loc=1.0, scale=0.25, size=x.shape)\n",
    "\n",
    "def test_trafo_Gauss_Sigma05(x):\n",
    "    return x*np.random.normal(loc=1.0, scale=0.5, size=x.shape)\n",
    "\n",
    "def test_trafo_Gauss_Sigma075(x):\n",
    "    return x*np.random.normal(loc=1.0, scale=0.75, size=x.shape)\n",
    "\n",
    "def test_trafo_Gauss_Sigma1(x):\n",
    "    return x*np.random.normal(loc=1.0, scale=1.0, size=x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c93ba8",
   "metadata": {},
   "source": [
    "## Trafo - 0.1 sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b61e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_in_len: 204032\n",
      "0 [21.80484] torch.Size([256, 1]) <class 'torch.Tensor'>\n",
      "32000 [56.13993] torch.Size([256, 1]) <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "sigma_path = '0,1' #for the outputpath\n",
    "\n",
    "input_filename = f\"/beegfs/desy/user/schreibj/Hadron_classifier/shower_data/pion_{int(n_gesamt/1000)}k_trafo.hdf5\"\n",
    "input_filename_validation = f\"/beegfs/desy/user/schreibj/Hadron_classifier/shower_data/pion_{int(n_gesamt/1000)}k_trafo_validation.hdf5\"\n",
    "input_filename_test = f\"/beegfs/desy/user/schreibj/Hadron_classifier/shower_data/pion_{int(n_gesamt/1000)}k_trafo_test.hdf5\"\n",
    "\n",
    "f_in = h5py.File(input_filename,'r')\n",
    "f_in_len = f_in['hcal_only']['layers'].shape[0]\n",
    "print(f'f_in_len: {f_in_len}')\n",
    "\n",
    "\n",
    "output_filename = f\"/beegfs/desy/user/schreibj/Hadron_classifier/shower_data/510k all/pion_{int(n_gesamt/1000)}k_trafo_{sigma_path}Sigma.hdf5\"\n",
    "f_out = h5py.File(output_filename,'w-')\n",
    "\n",
    "\n",
    "grp = f_out.create_group(\"hcal_only\")\n",
    "dset = grp.create_dataset('layers', shape=(0, 48, 48, 48), maxshape=(None, 48, 48, 48), \n",
    "                          chunks=(100, 48, 48, 48), dtype='f8', compression=\"gzip\", compression_opts=1)\n",
    "dsetE = grp.create_dataset('energy', shape=(0, 1), maxshape=(None, 1), chunks=(100, 1), \n",
    "                           dtype='f4', compression=\"gzip\", compression_opts=1)\n",
    "dsetL = grp.create_dataset('label', shape=(0, 1), maxshape=(None, 1), chunks=(100, 1), \n",
    "                           dtype='f4', compression=\"gzip\", compression_opts=1)\n",
    "\n",
    "\n",
    "for index in np.arange(0, f_in_len, step=batchsize):  #int(n_split1*training_percentage*0.01)\n",
    "    \n",
    "    ds1 = f_in['hcal_only']['layers'][index:index+batchsize]\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    # Trafo in showers (in ds1) here\n",
    "    \n",
    "    ds1 = test_trafo_Gauss_Sigma01(ds1)\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    dset.resize(dset.shape[0]+ds1.shape[0], axis=0)\n",
    "    dset[-1*ds1.shape[0]:] = ds1\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ds1E = f_in['hcal_only']['energy'][index:index+batchsize]   \n",
    "    \n",
    "    #######\n",
    "    \n",
    "    # Trafo in energy labels (in ds1E) here\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    dsetE.resize(dsetE.shape[0]+ds1E.shape[0], axis=0)\n",
    "    dsetE[-1*ds1E.shape[0]:] = ds1E\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ds1L = f_in['hcal_only']['label'][index:index+batchsize]\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    # Trafo in labels here\n",
    "    \n",
    "    \n",
    "    #######\n",
    "    \n",
    "    dsetL.resize(dsetL.shape[0]+ds1L.shape[0], axis=0)\n",
    "    dsetL[-1*ds1L.shape[0]:] = ds1L\n",
    "    ds1L = torch.from_numpy(ds1L)\n",
    "\n",
    "    if index%1000 == 0:\n",
    "        print(index, ds1E[0], ds1L.shape, type(ds1L))\n",
    "\n",
    "print('pion_trafo_test:')\n",
    "print(f_out['hcal_only']['layers'].shape[0])\n",
    "\n",
    "f_out.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "f_in_validation = h5py.File(input_filename_validation,'r')\n",
    "f_in_len_validation = f_in_validation['hcal_only']['layers'].shape[0]\n",
    "print(f'f_in_len_validation: {f_in_len_validation}')\n",
    "\n",
    "        \n",
    "output_filename_validation = f\"/beegfs/desy/user/schreibj/Hadron_classifier/shower_data/pion_{int(n_gesamt/1000)}k_trafo_{sigma_path}Sigma_validation.hdf5\"\n",
    "f_out_validation = h5py.File(output_filename_validation,'w-')\n",
    "\n",
    "\n",
    "grp = f_out_validation.create_group(\"hcal_only\")\n",
    "dset = grp.create_dataset('layers', shape=(0, 48, 48, 48), maxshape=(None, 48, 48, 48), \n",
    "                          chunks=(100, 48, 48, 48), dtype='f8', compression=\"gzip\", compression_opts=1)\n",
    "dsetE = grp.create_dataset('energy', shape=(0, 1), maxshape=(None, 1), chunks=(100, 1), \n",
    "                           dtype='f4', compression=\"gzip\", compression_opts=1)\n",
    "dsetL = grp.create_dataset('label', shape=(0, 1), maxshape=(None, 1), chunks=(100, 1), \n",
    "                           dtype='f4', compression=\"gzip\", compression_opts=1)\n",
    "\n",
    "\n",
    "for index in np.arange(0, f_in_len_validation, step=batchsize):\n",
    "    \n",
    "    ds1 = f_in_validation['hcal_only']['layers'][index:index+batchsize]\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    # Trafo in showers (in ds1) here\n",
    "    \n",
    "    ds1 = test_trafo_Gauss_Sigma01(ds1)\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    dset.resize(dset.shape[0]+ds1.shape[0], axis=0)\n",
    "    dset[-1*ds1.shape[0]:] = ds1\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ds1E = f_in_validation['hcal_only']['energy'][index:index+batchsize]   \n",
    "    \n",
    "    #######\n",
    "    \n",
    "    # Trafo in energy labels (in ds1E) here\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    dsetE.resize(dsetE.shape[0]+ds1E.shape[0], axis=0)\n",
    "    dsetE[-1*ds1E.shape[0]:] = ds1E\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ds1L = f_in_validation['hcal_only']['label'][index:index+batchsize]\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    # Trafo in labels here\n",
    "    \n",
    "    \n",
    "    #######\n",
    "    \n",
    "    dsetL.resize(dsetL.shape[0]+ds1L.shape[0], axis=0)\n",
    "    dsetL[-1*ds1L.shape[0]:] = ds1L\n",
    "    ds1L = torch.from_numpy(ds1L)\n",
    "\n",
    "    if index%1000 == 0:\n",
    "        print(index, ds1E[0], ds1L.shape, type(ds1L))\n",
    "\n",
    "print('pion_trafo_test:')\n",
    "print(f_out_validation['hcal_only']['layers'].shape[0])\n",
    "        \n",
    "f_out_validation.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "f_in_test = h5py.File(input_filename_test,'r')\n",
    "f_in_len_test = f_in_test['hcal_only']['layers'].shape[0]\n",
    "print(f'f_in_len_test: {f_in_len_test}')\n",
    "\n",
    "\n",
    "output_filename_test = f\"/beegfs/desy/user/schreibj/Hadron_classifier/shower_data/pion_{int(n_gesamt/1000)}k_trafo_{sigma_path}Sigma_test.hdf5\"\n",
    "f_out_test = h5py.File(output_filename_test,'w-')\n",
    "\n",
    "\n",
    "grp = f_out_test.create_group(\"hcal_only\")\n",
    "dset = grp.create_dataset('layers', shape=(0, 48, 48, 48), maxshape=(None, 48, 48, 48), \n",
    "                          chunks=(100, 48, 48, 48), dtype='f8', compression=\"gzip\", compression_opts=1)\n",
    "dsetE = grp.create_dataset('energy', shape=(0, 1), maxshape=(None, 1), chunks=(100, 1), \n",
    "                           dtype='f4', compression=\"gzip\", compression_opts=1)\n",
    "dsetL = grp.create_dataset('label', shape=(0, 1), maxshape=(None, 1), chunks=(100, 1), \n",
    "                           dtype='f4', compression=\"gzip\", compression_opts=1)\n",
    "\n",
    "\n",
    "for index in np.arange(0, f_in_len_test, step=batchsize):\n",
    "    \n",
    "    ds1 = f_in_test['hcal_only']['layers'][index:index+batchsize]\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    # Trafo in showers (in ds1) here\n",
    "    \n",
    "    ds1 = test_trafo_Gauss_Sigma01(ds1)\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    dset.resize(dset.shape[0]+ds1.shape[0], axis=0)\n",
    "    dset[-1*ds1.shape[0]:] = ds1\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ds1E = f_in_test['hcal_only']['energy'][index:index+batchsize]   \n",
    "    \n",
    "    #######\n",
    "    \n",
    "    # Trafo in energy labels (in ds1E) here\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    dsetE.resize(dsetE.shape[0]+ds1E.shape[0], axis=0)\n",
    "    dsetE[-1*ds1E.shape[0]:] = ds1E\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ds1L = f_in_test['hcal_only']['label'][index:index+batchsize]\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    # Trafo in labels here\n",
    "    \n",
    "    \n",
    "    #######\n",
    "    \n",
    "    dsetL.resize(dsetL.shape[0]+ds1L.shape[0], axis=0)\n",
    "    dsetL[-1*ds1L.shape[0]:] = ds1L\n",
    "    ds1L = torch.from_numpy(ds1L)\n",
    "\n",
    "    if index%1000 == 0:\n",
    "        print(index, ds1E[0], ds1L.shape, type(ds1L))\n",
    "\n",
    "print('pion_trafo_test:')\n",
    "print(f_out_test['hcal_only']['layers'].shape[0])\n",
    "        \n",
    "f_out_test.close()\n",
    "\n",
    "end = datetime.now()\n",
    "print(f'total time: {end - start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fa14db-b98e-4dab-a26b-98366bcb2351",
   "metadata": {},
   "source": [
    "## Trafo - 0.25 sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747f7dde-7164-4df8-b18e-3917cdae0af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "sigma_path = '0,25' #for the outputpath\n",
    "\n",
    "input_filename = f\"/beegfs/desy/user/schreibj/Hadron_classifier/shower_data/pion_{int(n_gesamt/1000)}k_trafo.hdf5\"\n",
    "input_filename_validation = f\"/beegfs/desy/user/schreibj/Hadron_classifier/shower_data/pion_{int(n_gesamt/1000)}k_trafo_validation.hdf5\"\n",
    "input_filename_test = f\"/beegfs/desy/user/schreibj/Hadron_classifier/shower_data/pion_{int(n_gesamt/1000)}k_trafo_test.hdf5\"\n",
    "\n",
    "f_in = h5py.File(input_filename,'r')\n",
    "f_in_len = f_in['hcal_only']['layers'].shape[0]\n",
    "print(f'f_in_len: {f_in_len}')\n",
    "\n",
    "\n",
    "output_filename = f\"/beegfs/desy/user/schreibj/Hadron_classifier/shower_data/510k all/pion_{int(n_gesamt/1000)}k_trafo_{sigma_path}Sigma.hdf5\"\n",
    "f_out = h5py.File(output_filename,'w-')\n",
    "\n",
    "\n",
    "grp = f_out.create_group(\"hcal_only\")\n",
    "dset = grp.create_dataset('layers', shape=(0, 48, 48, 48), maxshape=(None, 48, 48, 48), \n",
    "                          chunks=(100, 48, 48, 48), dtype='f8', compression=\"gzip\", compression_opts=1)\n",
    "dsetE = grp.create_dataset('energy', shape=(0, 1), maxshape=(None, 1), chunks=(100, 1), \n",
    "                           dtype='f4', compression=\"gzip\", compression_opts=1)\n",
    "dsetL = grp.create_dataset('label', shape=(0, 1), maxshape=(None, 1), chunks=(100, 1), \n",
    "                           dtype='f4', compression=\"gzip\", compression_opts=1)\n",
    "\n",
    "\n",
    "for index in np.arange(0, f_in_len, step=batchsize):  #int(n_split1*training_percentage*0.01)\n",
    "    \n",
    "    ds1 = f_in['hcal_only']['layers'][index:index+batchsize]\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    # Trafo in showers (in ds1) here\n",
    "    \n",
    "    ds1 = test_trafo_Gauss_Sigma025(ds1)\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    dset.resize(dset.shape[0]+ds1.shape[0], axis=0)\n",
    "    dset[-1*ds1.shape[0]:] = ds1\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ds1E = f_in['hcal_only']['energy'][index:index+batchsize]   \n",
    "    \n",
    "    #######\n",
    "    \n",
    "    # Trafo in energy labels (in ds1E) here\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    dsetE.resize(dsetE.shape[0]+ds1E.shape[0], axis=0)\n",
    "    dsetE[-1*ds1E.shape[0]:] = ds1E\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ds1L = f_in['hcal_only']['label'][index:index+batchsize]\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    # Trafo in labels here\n",
    "    \n",
    "    \n",
    "    #######\n",
    "    \n",
    "    dsetL.resize(dsetL.shape[0]+ds1L.shape[0], axis=0)\n",
    "    dsetL[-1*ds1L.shape[0]:] = ds1L\n",
    "    ds1L = torch.from_numpy(ds1L)\n",
    "\n",
    "    if index%1000 == 0:\n",
    "        print(index, ds1E[0], ds1L.shape, type(ds1L))\n",
    "\n",
    "print('pion_trafo_test:')\n",
    "print(f_out['hcal_only']['layers'].shape[0])\n",
    "\n",
    "f_out.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "f_in_validation = h5py.File(input_filename_validation,'r')\n",
    "f_in_len_validation = f_in_validation['hcal_only']['layers'].shape[0]\n",
    "print(f'f_in_len_validation: {f_in_len_validation}')\n",
    "\n",
    "        \n",
    "output_filename_validation = f\"/beegfs/desy/user/schreibj/Hadron_classifier/shower_data/pion_{int(n_gesamt/1000)}k_trafo_{sigma_path}Sigma_validation.hdf5\"\n",
    "f_out_validation = h5py.File(output_filename_validation,'w-')\n",
    "\n",
    "\n",
    "grp = f_out_validation.create_group(\"hcal_only\")\n",
    "dset = grp.create_dataset('layers', shape=(0, 48, 48, 48), maxshape=(None, 48, 48, 48), \n",
    "                          chunks=(100, 48, 48, 48), dtype='f8', compression=\"gzip\", compression_opts=1)\n",
    "dsetE = grp.create_dataset('energy', shape=(0, 1), maxshape=(None, 1), chunks=(100, 1), \n",
    "                           dtype='f4', compression=\"gzip\", compression_opts=1)\n",
    "dsetL = grp.create_dataset('label', shape=(0, 1), maxshape=(None, 1), chunks=(100, 1), \n",
    "                           dtype='f4', compression=\"gzip\", compression_opts=1)\n",
    "\n",
    "\n",
    "for index in np.arange(0, f_in_len_validation, step=batchsize):\n",
    "    \n",
    "    ds1 = f_in_validation['hcal_only']['layers'][index:index+batchsize]\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    # Trafo in showers (in ds1) here\n",
    "    \n",
    "    ds1 = test_trafo_Gauss_Sigma025(ds1)\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    dset.resize(dset.shape[0]+ds1.shape[0], axis=0)\n",
    "    dset[-1*ds1.shape[0]:] = ds1\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ds1E = f_in_validation['hcal_only']['energy'][index:index+batchsize]   \n",
    "    \n",
    "    #######\n",
    "    \n",
    "    # Trafo in energy labels (in ds1E) here\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    dsetE.resize(dsetE.shape[0]+ds1E.shape[0], axis=0)\n",
    "    dsetE[-1*ds1E.shape[0]:] = ds1E\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ds1L = f_in_validation['hcal_only']['label'][index:index+batchsize]\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    # Trafo in labels here\n",
    "    \n",
    "    \n",
    "    #######\n",
    "    \n",
    "    dsetL.resize(dsetL.shape[0]+ds1L.shape[0], axis=0)\n",
    "    dsetL[-1*ds1L.shape[0]:] = ds1L\n",
    "    ds1L = torch.from_numpy(ds1L)\n",
    "\n",
    "    if index%1000 == 0:\n",
    "        print(index, ds1E[0], ds1L.shape, type(ds1L))\n",
    "\n",
    "print('pion_trafo_test:')\n",
    "print(f_out_validation['hcal_only']['layers'].shape[0])\n",
    "        \n",
    "f_out_validation.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "f_in_test = h5py.File(input_filename_test,'r')\n",
    "f_in_len_test = f_in_test['hcal_only']['layers'].shape[0]\n",
    "print(f'f_in_len_test: {f_in_len_test}')\n",
    "\n",
    "\n",
    "output_filename_test = f\"/beegfs/desy/user/schreibj/Hadron_classifier/shower_data/pion_{int(n_gesamt/1000)}k_trafo_{sigma_path}Sigma_test.hdf5\"\n",
    "f_out_test = h5py.File(output_filename_test,'w-')\n",
    "\n",
    "\n",
    "grp = f_out_test.create_group(\"hcal_only\")\n",
    "dset = grp.create_dataset('layers', shape=(0, 48, 48, 48), maxshape=(None, 48, 48, 48), \n",
    "                          chunks=(100, 48, 48, 48), dtype='f8', compression=\"gzip\", compression_opts=1)\n",
    "dsetE = grp.create_dataset('energy', shape=(0, 1), maxshape=(None, 1), chunks=(100, 1), \n",
    "                           dtype='f4', compression=\"gzip\", compression_opts=1)\n",
    "dsetL = grp.create_dataset('label', shape=(0, 1), maxshape=(None, 1), chunks=(100, 1), \n",
    "                           dtype='f4', compression=\"gzip\", compression_opts=1)\n",
    "\n",
    "\n",
    "for index in np.arange(0, f_in_len_test, step=batchsize):\n",
    "    \n",
    "    ds1 = f_in_test['hcal_only']['layers'][index:index+batchsize]\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    # Trafo in showers (in ds1) here\n",
    "    \n",
    "    ds1 = test_trafo_Gauss_Sigma025(ds1)\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    dset.resize(dset.shape[0]+ds1.shape[0], axis=0)\n",
    "    dset[-1*ds1.shape[0]:] = ds1\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ds1E = f_in_test['hcal_only']['energy'][index:index+batchsize]   \n",
    "    \n",
    "    #######\n",
    "    \n",
    "    # Trafo in energy labels (in ds1E) here\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    dsetE.resize(dsetE.shape[0]+ds1E.shape[0], axis=0)\n",
    "    dsetE[-1*ds1E.shape[0]:] = ds1E\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ds1L = f_in_test['hcal_only']['label'][index:index+batchsize]\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    # Trafo in labels here\n",
    "    \n",
    "    \n",
    "    #######\n",
    "    \n",
    "    dsetL.resize(dsetL.shape[0]+ds1L.shape[0], axis=0)\n",
    "    dsetL[-1*ds1L.shape[0]:] = ds1L\n",
    "    ds1L = torch.from_numpy(ds1L)\n",
    "\n",
    "    if index%1000 == 0:\n",
    "        print(index, ds1E[0], ds1L.shape, type(ds1L))\n",
    "\n",
    "print('pion_trafo_test:')\n",
    "print(f_out_test['hcal_only']['layers'].shape[0])\n",
    "        \n",
    "f_out_test.close()\n",
    "\n",
    "end = datetime.now()\n",
    "print(f'total time: {end - start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5babbfd-c3a8-4551-9d74-4db80a8af8f1",
   "metadata": {},
   "source": [
    "## Trafo - 0.5 sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2c58ba-1446-4b6b-a589-ffd78af9fb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "sigma_path = '0,5' #for the outputpath\n",
    "\n",
    "input_filename = f\"/beegfs/desy/user/schreibj/Hadron_classifier/shower_data/pion_{int(n_gesamt/1000)}k_trafo.hdf5\"\n",
    "input_filename_validation = f\"/beegfs/desy/user/schreibj/Hadron_classifier/shower_data/pion_{int(n_gesamt/1000)}k_trafo_validation.hdf5\"\n",
    "input_filename_test = f\"/beegfs/desy/user/schreibj/Hadron_classifier/shower_data/pion_{int(n_gesamt/1000)}k_trafo_test.hdf5\"\n",
    "\n",
    "f_in = h5py.File(input_filename,'r')\n",
    "f_in_len = f_in['hcal_only']['layers'].shape[0]\n",
    "print(f'f_in_len: {f_in_len}')\n",
    "\n",
    "\n",
    "output_filename = f\"/beegfs/desy/user/schreibj/Hadron_classifier/shower_data/510k all/pion_{int(n_gesamt/1000)}k_trafo_{sigma_path}Sigma.hdf5\"\n",
    "f_out = h5py.File(output_filename,'w-')\n",
    "\n",
    "\n",
    "grp = f_out.create_group(\"hcal_only\")\n",
    "dset = grp.create_dataset('layers', shape=(0, 48, 48, 48), maxshape=(None, 48, 48, 48), \n",
    "                          chunks=(100, 48, 48, 48), dtype='f8', compression=\"gzip\", compression_opts=1)\n",
    "dsetE = grp.create_dataset('energy', shape=(0, 1), maxshape=(None, 1), chunks=(100, 1), \n",
    "                           dtype='f4', compression=\"gzip\", compression_opts=1)\n",
    "dsetL = grp.create_dataset('label', shape=(0, 1), maxshape=(None, 1), chunks=(100, 1), \n",
    "                           dtype='f4', compression=\"gzip\", compression_opts=1)\n",
    "\n",
    "\n",
    "for index in np.arange(0, f_in_len, step=batchsize):  #int(n_split1*training_percentage*0.01)\n",
    "    \n",
    "    ds1 = f_in['hcal_only']['layers'][index:index+batchsize]\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    # Trafo in showers (in ds1) here\n",
    "    \n",
    "    ds1 = test_trafo_Gauss_Sigma05(ds1)\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    dset.resize(dset.shape[0]+ds1.shape[0], axis=0)\n",
    "    dset[-1*ds1.shape[0]:] = ds1\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ds1E = f_in['hcal_only']['energy'][index:index+batchsize]   \n",
    "    \n",
    "    #######\n",
    "    \n",
    "    # Trafo in energy labels (in ds1E) here\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    dsetE.resize(dsetE.shape[0]+ds1E.shape[0], axis=0)\n",
    "    dsetE[-1*ds1E.shape[0]:] = ds1E\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ds1L = f_in['hcal_only']['label'][index:index+batchsize]\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    # Trafo in labels here\n",
    "    \n",
    "    \n",
    "    #######\n",
    "    \n",
    "    dsetL.resize(dsetL.shape[0]+ds1L.shape[0], axis=0)\n",
    "    dsetL[-1*ds1L.shape[0]:] = ds1L\n",
    "    ds1L = torch.from_numpy(ds1L)\n",
    "\n",
    "    if index%1000 == 0:\n",
    "        print(index, ds1E[0], ds1L.shape, type(ds1L))\n",
    "\n",
    "print('pion_trafo_test:')\n",
    "print(f_out['hcal_only']['layers'].shape[0])\n",
    "\n",
    "f_out.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "f_in_validation = h5py.File(input_filename_validation,'r')\n",
    "f_in_len_validation = f_in_validation['hcal_only']['layers'].shape[0]\n",
    "print(f'f_in_len_validation: {f_in_len_validation}')\n",
    "\n",
    "        \n",
    "output_filename_validation = f\"/beegfs/desy/user/schreibj/Hadron_classifier/shower_data/pion_{int(n_gesamt/1000)}k_trafo_{sigma_path}Sigma_validation.hdf5\"\n",
    "f_out_validation = h5py.File(output_filename_validation,'w-')\n",
    "\n",
    "\n",
    "grp = f_out_validation.create_group(\"hcal_only\")\n",
    "dset = grp.create_dataset('layers', shape=(0, 48, 48, 48), maxshape=(None, 48, 48, 48), \n",
    "                          chunks=(100, 48, 48, 48), dtype='f8', compression=\"gzip\", compression_opts=1)\n",
    "dsetE = grp.create_dataset('energy', shape=(0, 1), maxshape=(None, 1), chunks=(100, 1), \n",
    "                           dtype='f4', compression=\"gzip\", compression_opts=1)\n",
    "dsetL = grp.create_dataset('label', shape=(0, 1), maxshape=(None, 1), chunks=(100, 1), \n",
    "                           dtype='f4', compression=\"gzip\", compression_opts=1)\n",
    "\n",
    "\n",
    "for index in np.arange(0, f_in_len_validation, step=batchsize):\n",
    "    \n",
    "    ds1 = f_in_validation['hcal_only']['layers'][index:index+batchsize]\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    # Trafo in showers (in ds1) here\n",
    "    \n",
    "    ds1 = test_trafo_Gauss_Sigma05(ds1)\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    dset.resize(dset.shape[0]+ds1.shape[0], axis=0)\n",
    "    dset[-1*ds1.shape[0]:] = ds1\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ds1E = f_in_validation['hcal_only']['energy'][index:index+batchsize]   \n",
    "    \n",
    "    #######\n",
    "    \n",
    "    # Trafo in energy labels (in ds1E) here\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    dsetE.resize(dsetE.shape[0]+ds1E.shape[0], axis=0)\n",
    "    dsetE[-1*ds1E.shape[0]:] = ds1E\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ds1L = f_in_validation['hcal_only']['label'][index:index+batchsize]\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    # Trafo in labels here\n",
    "    \n",
    "    \n",
    "    #######\n",
    "    \n",
    "    dsetL.resize(dsetL.shape[0]+ds1L.shape[0], axis=0)\n",
    "    dsetL[-1*ds1L.shape[0]:] = ds1L\n",
    "    ds1L = torch.from_numpy(ds1L)\n",
    "\n",
    "    if index%1000 == 0:\n",
    "        print(index, ds1E[0], ds1L.shape, type(ds1L))\n",
    "\n",
    "print('pion_trafo_test:')\n",
    "print(f_out_validation['hcal_only']['layers'].shape[0])\n",
    "        \n",
    "f_out_validation.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "f_in_test = h5py.File(input_filename_test,'r')\n",
    "f_in_len_test = f_in_test['hcal_only']['layers'].shape[0]\n",
    "print(f'f_in_len_test: {f_in_len_test}')\n",
    "\n",
    "\n",
    "output_filename_test = f\"/beegfs/desy/user/schreibj/Hadron_classifier/shower_data/pion_{int(n_gesamt/1000)}k_trafo_{sigma_path}Sigma_test.hdf5\"\n",
    "f_out_test = h5py.File(output_filename_test,'w-')\n",
    "\n",
    "\n",
    "grp = f_out_test.create_group(\"hcal_only\")\n",
    "dset = grp.create_dataset('layers', shape=(0, 48, 48, 48), maxshape=(None, 48, 48, 48), \n",
    "                          chunks=(100, 48, 48, 48), dtype='f8', compression=\"gzip\", compression_opts=1)\n",
    "dsetE = grp.create_dataset('energy', shape=(0, 1), maxshape=(None, 1), chunks=(100, 1), \n",
    "                           dtype='f4', compression=\"gzip\", compression_opts=1)\n",
    "dsetL = grp.create_dataset('label', shape=(0, 1), maxshape=(None, 1), chunks=(100, 1), \n",
    "                           dtype='f4', compression=\"gzip\", compression_opts=1)\n",
    "\n",
    "\n",
    "for index in np.arange(0, f_in_len_test, step=batchsize):\n",
    "    \n",
    "    ds1 = f_in_test['hcal_only']['layers'][index:index+batchsize]\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    # Trafo in showers (in ds1) here\n",
    "    \n",
    "    ds1 = test_trafo_Gauss_Sigma05(ds1)\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    dset.resize(dset.shape[0]+ds1.shape[0], axis=0)\n",
    "    dset[-1*ds1.shape[0]:] = ds1\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ds1E = f_in_test['hcal_only']['energy'][index:index+batchsize]   \n",
    "    \n",
    "    #######\n",
    "    \n",
    "    # Trafo in energy labels (in ds1E) here\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    dsetE.resize(dsetE.shape[0]+ds1E.shape[0], axis=0)\n",
    "    dsetE[-1*ds1E.shape[0]:] = ds1E\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ds1L = f_in_test['hcal_only']['label'][index:index+batchsize]\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    # Trafo in labels here\n",
    "    \n",
    "    \n",
    "    #######\n",
    "    \n",
    "    dsetL.resize(dsetL.shape[0]+ds1L.shape[0], axis=0)\n",
    "    dsetL[-1*ds1L.shape[0]:] = ds1L\n",
    "    ds1L = torch.from_numpy(ds1L)\n",
    "\n",
    "    if index%1000 == 0:\n",
    "        print(index, ds1E[0], ds1L.shape, type(ds1L))\n",
    "\n",
    "print('pion_trafo_test:')\n",
    "print(f_out_test['hcal_only']['layers'].shape[0])\n",
    "        \n",
    "f_out_test.close()\n",
    "\n",
    "end = datetime.now()\n",
    "print(f'total time: {end - start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d465da6a-a572-4dc2-b607-64b6e2358cb7",
   "metadata": {},
   "source": [
    "## Trafo - 0.75 sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c14a9f-330d-4d4a-b8b4-968d255c0682",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "sigma_path = '0,75' #for the outputpath\n",
    "\n",
    "input_filename = f\"/beegfs/desy/user/schreibj/Hadron_classifier/shower_data/pion_{int(n_gesamt/1000)}k_trafo.hdf5\"\n",
    "input_filename_validation = f\"/beegfs/desy/user/schreibj/Hadron_classifier/shower_data/pion_{int(n_gesamt/1000)}k_trafo_validation.hdf5\"\n",
    "input_filename_test = f\"/beegfs/desy/user/schreibj/Hadron_classifier/shower_data/pion_{int(n_gesamt/1000)}k_trafo_test.hdf5\"\n",
    "\n",
    "f_in = h5py.File(input_filename,'r')\n",
    "f_in_len = f_in['hcal_only']['layers'].shape[0]\n",
    "print(f'f_in_len: {f_in_len}')\n",
    "\n",
    "\n",
    "output_filename = f\"/beegfs/desy/user/schreibj/Hadron_classifier/shower_data/510k all/pion_{int(n_gesamt/1000)}k_trafo_{sigma_path}Sigma.hdf5\"\n",
    "f_out = h5py.File(output_filename,'w-')\n",
    "\n",
    "\n",
    "grp = f_out.create_group(\"hcal_only\")\n",
    "dset = grp.create_dataset('layers', shape=(0, 48, 48, 48), maxshape=(None, 48, 48, 48), \n",
    "                          chunks=(100, 48, 48, 48), dtype='f8', compression=\"gzip\", compression_opts=1)\n",
    "dsetE = grp.create_dataset('energy', shape=(0, 1), maxshape=(None, 1), chunks=(100, 1), \n",
    "                           dtype='f4', compression=\"gzip\", compression_opts=1)\n",
    "dsetL = grp.create_dataset('label', shape=(0, 1), maxshape=(None, 1), chunks=(100, 1), \n",
    "                           dtype='f4', compression=\"gzip\", compression_opts=1)\n",
    "\n",
    "\n",
    "for index in np.arange(0, f_in_len, step=batchsize):  #int(n_split1*training_percentage*0.01)\n",
    "    \n",
    "    ds1 = f_in['hcal_only']['layers'][index:index+batchsize]\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    # Trafo in showers (in ds1) here\n",
    "    \n",
    "    ds1 = test_trafo_Gauss_Sigma075(ds1)\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    dset.resize(dset.shape[0]+ds1.shape[0], axis=0)\n",
    "    dset[-1*ds1.shape[0]:] = ds1\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ds1E = f_in['hcal_only']['energy'][index:index+batchsize]   \n",
    "    \n",
    "    #######\n",
    "    \n",
    "    # Trafo in energy labels (in ds1E) here\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    dsetE.resize(dsetE.shape[0]+ds1E.shape[0], axis=0)\n",
    "    dsetE[-1*ds1E.shape[0]:] = ds1E\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ds1L = f_in['hcal_only']['label'][index:index+batchsize]\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    # Trafo in labels here\n",
    "    \n",
    "    \n",
    "    #######\n",
    "    \n",
    "    dsetL.resize(dsetL.shape[0]+ds1L.shape[0], axis=0)\n",
    "    dsetL[-1*ds1L.shape[0]:] = ds1L\n",
    "    ds1L = torch.from_numpy(ds1L)\n",
    "\n",
    "    if index%1000 == 0:\n",
    "        print(index, ds1E[0], ds1L.shape, type(ds1L))\n",
    "\n",
    "print('pion_trafo_test:')\n",
    "print(f_out['hcal_only']['layers'].shape[0])\n",
    "\n",
    "f_out.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "f_in_validation = h5py.File(input_filename_validation,'r')\n",
    "f_in_len_validation = f_in_validation['hcal_only']['layers'].shape[0]\n",
    "print(f'f_in_len_validation: {f_in_len_validation}')\n",
    "\n",
    "        \n",
    "output_filename_validation = f\"/beegfs/desy/user/schreibj/Hadron_classifier/shower_data/pion_{int(n_gesamt/1000)}k_trafo_{sigma_path}Sigma_validation.hdf5\"\n",
    "f_out_validation = h5py.File(output_filename_validation,'w-')\n",
    "\n",
    "\n",
    "grp = f_out_validation.create_group(\"hcal_only\")\n",
    "dset = grp.create_dataset('layers', shape=(0, 48, 48, 48), maxshape=(None, 48, 48, 48), \n",
    "                          chunks=(100, 48, 48, 48), dtype='f8', compression=\"gzip\", compression_opts=1)\n",
    "dsetE = grp.create_dataset('energy', shape=(0, 1), maxshape=(None, 1), chunks=(100, 1), \n",
    "                           dtype='f4', compression=\"gzip\", compression_opts=1)\n",
    "dsetL = grp.create_dataset('label', shape=(0, 1), maxshape=(None, 1), chunks=(100, 1), \n",
    "                           dtype='f4', compression=\"gzip\", compression_opts=1)\n",
    "\n",
    "\n",
    "for index in np.arange(0, f_in_len_validation, step=batchsize):\n",
    "    \n",
    "    ds1 = f_in_validation['hcal_only']['layers'][index:index+batchsize]\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    # Trafo in showers (in ds1) here\n",
    "    \n",
    "    ds1 = test_trafo_Gauss_Sigma075(ds1)\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    dset.resize(dset.shape[0]+ds1.shape[0], axis=0)\n",
    "    dset[-1*ds1.shape[0]:] = ds1\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ds1E = f_in_validation['hcal_only']['energy'][index:index+batchsize]   \n",
    "    \n",
    "    #######\n",
    "    \n",
    "    # Trafo in energy labels (in ds1E) here\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    dsetE.resize(dsetE.shape[0]+ds1E.shape[0], axis=0)\n",
    "    dsetE[-1*ds1E.shape[0]:] = ds1E\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ds1L = f_in_validation['hcal_only']['label'][index:index+batchsize]\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    # Trafo in labels here\n",
    "    \n",
    "    \n",
    "    #######\n",
    "    \n",
    "    dsetL.resize(dsetL.shape[0]+ds1L.shape[0], axis=0)\n",
    "    dsetL[-1*ds1L.shape[0]:] = ds1L\n",
    "    ds1L = torch.from_numpy(ds1L)\n",
    "\n",
    "    if index%1000 == 0:\n",
    "        print(index, ds1E[0], ds1L.shape, type(ds1L))\n",
    "\n",
    "print('pion_trafo_test:')\n",
    "print(f_out_validation['hcal_only']['layers'].shape[0])\n",
    "        \n",
    "f_out_validation.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "f_in_test = h5py.File(input_filename_test,'r')\n",
    "f_in_len_test = f_in_test['hcal_only']['layers'].shape[0]\n",
    "print(f'f_in_len_test: {f_in_len_test}')\n",
    "\n",
    "\n",
    "output_filename_test = f\"/beegfs/desy/user/schreibj/Hadron_classifier/shower_data/pion_{int(n_gesamt/1000)}k_trafo_{sigma_path}Sigma_test.hdf5\"\n",
    "f_out_test = h5py.File(output_filename_test,'w-')\n",
    "\n",
    "\n",
    "grp = f_out_test.create_group(\"hcal_only\")\n",
    "dset = grp.create_dataset('layers', shape=(0, 48, 48, 48), maxshape=(None, 48, 48, 48), \n",
    "                          chunks=(100, 48, 48, 48), dtype='f8', compression=\"gzip\", compression_opts=1)\n",
    "dsetE = grp.create_dataset('energy', shape=(0, 1), maxshape=(None, 1), chunks=(100, 1), \n",
    "                           dtype='f4', compression=\"gzip\", compression_opts=1)\n",
    "dsetL = grp.create_dataset('label', shape=(0, 1), maxshape=(None, 1), chunks=(100, 1), \n",
    "                           dtype='f4', compression=\"gzip\", compression_opts=1)\n",
    "\n",
    "\n",
    "for index in np.arange(0, f_in_len_test, step=batchsize):\n",
    "    \n",
    "    ds1 = f_in_test['hcal_only']['layers'][index:index+batchsize]\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    # Trafo in showers (in ds1) here\n",
    "    \n",
    "    ds1 = test_trafo_Gauss_Sigma075(ds1)\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    dset.resize(dset.shape[0]+ds1.shape[0], axis=0)\n",
    "    dset[-1*ds1.shape[0]:] = ds1\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ds1E = f_in_test['hcal_only']['energy'][index:index+batchsize]   \n",
    "    \n",
    "    #######\n",
    "    \n",
    "    # Trafo in energy labels (in ds1E) here\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    dsetE.resize(dsetE.shape[0]+ds1E.shape[0], axis=0)\n",
    "    dsetE[-1*ds1E.shape[0]:] = ds1E\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ds1L = f_in_test['hcal_only']['label'][index:index+batchsize]\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    # Trafo in labels here\n",
    "    \n",
    "    \n",
    "    #######\n",
    "    \n",
    "    dsetL.resize(dsetL.shape[0]+ds1L.shape[0], axis=0)\n",
    "    dsetL[-1*ds1L.shape[0]:] = ds1L\n",
    "    ds1L = torch.from_numpy(ds1L)\n",
    "\n",
    "    if index%1000 == 0:\n",
    "        print(index, ds1E[0], ds1L.shape, type(ds1L))\n",
    "\n",
    "print('pion_trafo_test:')\n",
    "print(f_out_test['hcal_only']['layers'].shape[0])\n",
    "        \n",
    "f_out_test.close()\n",
    "\n",
    "end = datetime.now()\n",
    "print(f'total time: {end - start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94b60b4-1db7-4234-ab24-b5131e4ec5eb",
   "metadata": {},
   "source": [
    "## Trafo - 1.0 sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71efdf8b-50c1-48a5-9a1f-fac131a41b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "sigma_path = '1,0' #for the outputpath\n",
    "\n",
    "input_filename = f\"/beegfs/desy/user/schreibj/Hadron_classifier/shower_data/pion_{int(n_gesamt/1000)}k_trafo.hdf5\"\n",
    "input_filename_validation = f\"/beegfs/desy/user/schreibj/Hadron_classifier/shower_data/pion_{int(n_gesamt/1000)}k_trafo_validation.hdf5\"\n",
    "input_filename_test = f\"/beegfs/desy/user/schreibj/Hadron_classifier/shower_data/pion_{int(n_gesamt/1000)}k_trafo_test.hdf5\"\n",
    "\n",
    "f_in = h5py.File(input_filename,'r')\n",
    "f_in_len = f_in['hcal_only']['layers'].shape[0]\n",
    "print(f'f_in_len: {f_in_len}')\n",
    "\n",
    "\n",
    "output_filename = f\"/beegfs/desy/user/schreibj/Hadron_classifier/shower_data/510k all/pion_{int(n_gesamt/1000)}k_trafo_{sigma_path}Sigma.hdf5\"\n",
    "f_out = h5py.File(output_filename,'w-')\n",
    "\n",
    "\n",
    "grp = f_out.create_group(\"hcal_only\")\n",
    "dset = grp.create_dataset('layers', shape=(0, 48, 48, 48), maxshape=(None, 48, 48, 48), \n",
    "                          chunks=(100, 48, 48, 48), dtype='f8', compression=\"gzip\", compression_opts=1)\n",
    "dsetE = grp.create_dataset('energy', shape=(0, 1), maxshape=(None, 1), chunks=(100, 1), \n",
    "                           dtype='f4', compression=\"gzip\", compression_opts=1)\n",
    "dsetL = grp.create_dataset('label', shape=(0, 1), maxshape=(None, 1), chunks=(100, 1), \n",
    "                           dtype='f4', compression=\"gzip\", compression_opts=1)\n",
    "\n",
    "\n",
    "for index in np.arange(0, f_in_len, step=batchsize):  #int(n_split1*training_percentage*0.01)\n",
    "    \n",
    "    ds1 = f_in['hcal_only']['layers'][index:index+batchsize]\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    # Trafo in showers (in ds1) here\n",
    "    \n",
    "    ds1 = test_trafo_Gauss_Sigma1(ds1)\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    dset.resize(dset.shape[0]+ds1.shape[0], axis=0)\n",
    "    dset[-1*ds1.shape[0]:] = ds1\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ds1E = f_in['hcal_only']['energy'][index:index+batchsize]   \n",
    "    \n",
    "    #######\n",
    "    \n",
    "    # Trafo in energy labels (in ds1E) here\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    dsetE.resize(dsetE.shape[0]+ds1E.shape[0], axis=0)\n",
    "    dsetE[-1*ds1E.shape[0]:] = ds1E\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ds1L = f_in['hcal_only']['label'][index:index+batchsize]\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    # Trafo in labels here\n",
    "    \n",
    "    \n",
    "    #######\n",
    "    \n",
    "    dsetL.resize(dsetL.shape[0]+ds1L.shape[0], axis=0)\n",
    "    dsetL[-1*ds1L.shape[0]:] = ds1L\n",
    "    ds1L = torch.from_numpy(ds1L)\n",
    "\n",
    "    if index%1000 == 0:\n",
    "        print(index, ds1E[0], ds1L.shape, type(ds1L))\n",
    "\n",
    "print('pion_trafo_test:')\n",
    "print(f_out['hcal_only']['layers'].shape[0])\n",
    "\n",
    "f_out.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "f_in_validation = h5py.File(input_filename_validation,'r')\n",
    "f_in_len_validation = f_in_validation['hcal_only']['layers'].shape[0]\n",
    "print(f'f_in_len_validation: {f_in_len_validation}')\n",
    "\n",
    "        \n",
    "output_filename_validation = f\"/beegfs/desy/user/schreibj/Hadron_classifier/shower_data/pion_{int(n_gesamt/1000)}k_trafo_{sigma_path}Sigma_validation.hdf5\"\n",
    "f_out_validation = h5py.File(output_filename_validation,'w-')\n",
    "\n",
    "\n",
    "grp = f_out_validation.create_group(\"hcal_only\")\n",
    "dset = grp.create_dataset('layers', shape=(0, 48, 48, 48), maxshape=(None, 48, 48, 48), \n",
    "                          chunks=(100, 48, 48, 48), dtype='f8', compression=\"gzip\", compression_opts=1)\n",
    "dsetE = grp.create_dataset('energy', shape=(0, 1), maxshape=(None, 1), chunks=(100, 1), \n",
    "                           dtype='f4', compression=\"gzip\", compression_opts=1)\n",
    "dsetL = grp.create_dataset('label', shape=(0, 1), maxshape=(None, 1), chunks=(100, 1), \n",
    "                           dtype='f4', compression=\"gzip\", compression_opts=1)\n",
    "\n",
    "\n",
    "for index in np.arange(0, f_in_len_validation, step=batchsize):\n",
    "    \n",
    "    ds1 = f_in_validation['hcal_only']['layers'][index:index+batchsize]\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    # Trafo in showers (in ds1) here\n",
    "    \n",
    "    ds1 = test_trafo_Gauss_Sigma1(ds1)\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    dset.resize(dset.shape[0]+ds1.shape[0], axis=0)\n",
    "    dset[-1*ds1.shape[0]:] = ds1\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ds1E = f_in_validation['hcal_only']['energy'][index:index+batchsize]   \n",
    "    \n",
    "    #######\n",
    "    \n",
    "    # Trafo in energy labels (in ds1E) here\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    dsetE.resize(dsetE.shape[0]+ds1E.shape[0], axis=0)\n",
    "    dsetE[-1*ds1E.shape[0]:] = ds1E\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ds1L = f_in_validation['hcal_only']['label'][index:index+batchsize]\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    # Trafo in labels here\n",
    "    \n",
    "    \n",
    "    #######\n",
    "    \n",
    "    dsetL.resize(dsetL.shape[0]+ds1L.shape[0], axis=0)\n",
    "    dsetL[-1*ds1L.shape[0]:] = ds1L\n",
    "    ds1L = torch.from_numpy(ds1L)\n",
    "\n",
    "    if index%1000 == 0:\n",
    "        print(index, ds1E[0], ds1L.shape, type(ds1L))\n",
    "\n",
    "print('pion_trafo_test:')\n",
    "print(f_out_validation['hcal_only']['layers'].shape[0])\n",
    "        \n",
    "f_out_validation.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "f_in_test = h5py.File(input_filename_test,'r')\n",
    "f_in_len_test = f_in_test['hcal_only']['layers'].shape[0]\n",
    "print(f'f_in_len_test: {f_in_len_test}')\n",
    "\n",
    "\n",
    "output_filename_test = f\"/beegfs/desy/user/schreibj/Hadron_classifier/shower_data/pion_{int(n_gesamt/1000)}k_trafo_{sigma_path}Sigma_test.hdf5\"\n",
    "f_out_test = h5py.File(output_filename_test,'w-')\n",
    "\n",
    "\n",
    "grp = f_out_test.create_group(\"hcal_only\")\n",
    "dset = grp.create_dataset('layers', shape=(0, 48, 48, 48), maxshape=(None, 48, 48, 48), \n",
    "                          chunks=(100, 48, 48, 48), dtype='f8', compression=\"gzip\", compression_opts=1)\n",
    "dsetE = grp.create_dataset('energy', shape=(0, 1), maxshape=(None, 1), chunks=(100, 1), \n",
    "                           dtype='f4', compression=\"gzip\", compression_opts=1)\n",
    "dsetL = grp.create_dataset('label', shape=(0, 1), maxshape=(None, 1), chunks=(100, 1), \n",
    "                           dtype='f4', compression=\"gzip\", compression_opts=1)\n",
    "\n",
    "\n",
    "for index in np.arange(0, f_in_len_test, step=batchsize):\n",
    "    \n",
    "    ds1 = f_in_test['hcal_only']['layers'][index:index+batchsize]\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    # Trafo in showers (in ds1) here\n",
    "    \n",
    "    ds1 = test_trafo_Gauss_Sigma1(ds1)\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    dset.resize(dset.shape[0]+ds1.shape[0], axis=0)\n",
    "    dset[-1*ds1.shape[0]:] = ds1\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ds1E = f_in_test['hcal_only']['energy'][index:index+batchsize]   \n",
    "    \n",
    "    #######\n",
    "    \n",
    "    # Trafo in energy labels (in ds1E) here\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    dsetE.resize(dsetE.shape[0]+ds1E.shape[0], axis=0)\n",
    "    dsetE[-1*ds1E.shape[0]:] = ds1E\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ds1L = f_in_test['hcal_only']['label'][index:index+batchsize]\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    # Trafo in labels here\n",
    "    \n",
    "    \n",
    "    #######\n",
    "    \n",
    "    dsetL.resize(dsetL.shape[0]+ds1L.shape[0], axis=0)\n",
    "    dsetL[-1*ds1L.shape[0]:] = ds1L\n",
    "    ds1L = torch.from_numpy(ds1L)\n",
    "\n",
    "    if index%1000 == 0:\n",
    "        print(index, ds1E[0], ds1L.shape, type(ds1L))\n",
    "\n",
    "print('pion_trafo_test:')\n",
    "print(f_out_test['hcal_only']['layers'].shape[0])\n",
    "        \n",
    "f_out_test.close()\n",
    "\n",
    "end = datetime.now()\n",
    "print(f'total time: {end - start}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "customStuff",
   "language": "python",
   "name": "customconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
